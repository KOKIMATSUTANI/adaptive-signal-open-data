
# CDDDQN: Constrained Deep Double Dueling Q-Network

A PyTorch implementation of CDDDQN for constrained reinforcement learning in grid environments.

## ğŸ¯ Overview

This project implements a **Constrained Deep Double Dueling Q-Network (CDDDQN)** that learns to navigate a 5Ã—5 grid world while avoiding hazards and maximizing rewards.

## ğŸ—ï¸ Architecture

- **Environment**: 5Ã—5 GridWorld with walls and hazards
- **Actions**: 4-directional movement (Up, Right, Down, Left)
- **State**: Normalized coordinates [0,1]
- **Reward**: +10 for goal, -1 per step
- **Cost**: +1 for hazard tiles

## ğŸ§  Key Features

- âœ… **Invalid Action Masking (IAM)** - Prevents invalid moves
- âœ… **Prioritized Experience Replay (PER)** - Efficient learning
- âœ… **Dueling Architecture** - Separate value and advantage streams
- âœ… **Double DQN** - Stable target network updates
- âœ… **Lagrangian Constraint** - Balances reward vs. cost optimization
- âœ… **EMA Cost Estimation** - Adaptive constraint handling

## ğŸš€ Quick Start

```bash
# Install dependencies
pip install torch matplotlib numpy

# Run training
python cdddqn_min.py
```

## ğŸ“Š Results

The algorithm learns to:
- Navigate from start (0,0) to goal (4,4)
- Avoid hazard tiles while minimizing path length
- Balance exploration vs. exploitation

## ğŸ® Environment Layout

```
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚  S  â”‚     â”‚     â”‚  H  â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚     â”‚  W  â”‚  W  â”‚     â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚     â”‚  W  â”‚  H  â”‚     â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚     â”‚     â”‚     â”‚     â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚     â”‚     â”‚     â”‚     â”‚  G  â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

S = Start, G = Goal, W = Wall, H = Hazard
```

## ğŸ“ˆ Training Output

- Episode Reward Plot
- Episode Cost Plot  
- Lambda & Cost Estimation Plot

## ğŸ”§ Parameters

- **Training Steps**: 4,000
- **Batch Size**: 32
- **Learning Rate**: 1e-3
- **Gamma**: 0.99
- **Epsilon**: 0.9 â†’ 0.05
- **Lambda LR**: 3e-3
- **Cost Budget**: 0.10

## ğŸ“š References

- [Double DQN](https://arxiv.org/abs/1509.06461)
- [Dueling DQN](https://arxiv.org/abs/1511.06581)
- [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)

## ğŸ“„ License

MIT License - see LICENSE file for details.